# The Symbiosis of Deep Learning and Differential Equations


The focus of this workshop is on the interplay between deep learning (DL) and differential equations (DEs).
In recent years, there has been a rapid increase of machine learning applications in computational sciences, with some of the most impressive results at the interface of DL and DEs. 
These successes have widespread implications, as DEs are among the most well-understood tools for the mathematical analysis of scientific knowledge, and they are fundamental building blocks for mathematical models in engineering, finance, and the natural sciences. 
This relationship is mutually beneficial.
DL techniques have been used in a variety of ways to dramatically enhance the effectiveness of DE solvers and computer simulations.
Conversely, DEs have also been used as mathematical models of the neural architectures and training algorithms arising in DL.

This workshop will aim to bring together researchers from each discipline to encourage intellectual exchanges and cultivate relationships between the two communities.
The scope of the workshop will include important topics at the intersection of DL and DEs, such as:

- How can DE models provide insights into DL?
  - What families of functions are best represented by different neural architectures?
  - Can this connection guide the design of new neural architectures?
  - Can DE models be used to derive bounds on generalization error?
  - What insights can DE models provide into training dynamics?
  - Can these insights guide the design of weight initialization schemes?    

- How can DL be used to enhance the analysis of DEs?
  - Solving high dimensional DEs (e.g., many-body physics, multi-agent models, …)
  - Solving highly parameterized DEs 
  - Solving inverse problems
  - Solving DEs with irregular solutions (exhibiting e.g., singularities, shocks, …)



We expect to publish around 40 articles, out of 80 submissions, and 200 attendees. We plan to crowdsource revisions so every main author to submit an article is expected to review three submissions. The submission date for Workshop contributions is Sep 17, 2021 and the  Accept/Reject notification date is  Oct 17, 2021.


| EST (Morning) |                                  | EST (Afternoon) |                     |
|---------------|----------------------------------|-----------------|---------------------|
| 08:00         | Introduction and opening remarks |                 |                     |
| 08:15         | Invited Talk 1                   | 13:15           | Invited Talk 3      |
| 09:00         | Contributed Talk 1               | 14:00           | Contributed Talk 2  |
| 09:15         | Coffee Break                     | 14:15           | Coffee Break        |
| 09:20         | Poster Spotlights                | 14:20           | Poster Spotlights 2 |
| 09:30         | Poster Session 1                 | 14:30           | Poster Session 2    |
| 10:15         | Invited Talk 2                   | 15:15           | Invited Talk 4      |
| 11:00         | Coffee Break                     | 16:00           | Contributed Talk 3  |
| 11:05         | Panel discussion                 | 16:15           | Contributed Talk 4  |
| 12:15         | Lunch Break                      | 16:30           | End                 |


Four of the papers submitted to the workshop will be selected as 15-minute contributed talks.
Our tentative schedule is presented in detail in the table above. We will live-stream all sessions during the workshop through Zoom. 
We will also encourage participation via online media, such as Twitter and
Reddit. 


## Speakers

[Neha Yadav](https://portfolios.nith.ac.in/index.php?/nith/dr-neha-yadav-) (confirmed) is an assistant professor at the National Institute of Technology Hamirpur. Her research focuses on using neural networks and other machine learning techniques to solve differential equations, and exploring new optimization algorithms such as harmony search algorithms or particle swarm optimization. In addition to her many publications, she is a co-author of ``An Introduction to Neural Network Methods for Differential Equations,'' the first textbook on the subject. 


[Philipp Grohs](https://mat.univie.ac.at/~grohs/) (confirmed) is a professor at the University of Vienna. His research interests lie in approximation theory and computational harmonic analysis. In addition, he is known for his work on numerical methods for PDEs, and has made important contributions to the development and analysis of machine learning algorithms for the numerical approximation of high-dimensional PDEs. Of particular notice is his theoretical work on error estimates for deep network approximations to the solutions of PDEs, including his proofs that neural networks overcome the curse of dimensionality in these applications. 

[Weinan E](https://web.math.princeton.edu/~weinan/) (confirmed) is a professor in Applied and Computational Mathematics at Princeton University. His current research interests lie at the intersection of mathematics, physics, and machine learning. He has made important contributions to the theoretical foundations of deep neural networks, exploring questions on training dynamics and generalization. Additionally, he has driven the development of machine learning algorithms for use in computational science, including solution methods for complicated PDE problems and techniques for incorporating mathematical models into data-driven approaches. 

[Anima Anandkumar](http://tensorlab.cms.caltech.edu/users/anima/) (confirmed) is a professor in Applied and Computational Mathematics at Princeton University. Her current research interests lie at the intersection of mathematics, physics, and machine learning. She has made important contributions to the theoretical foundations of deep neural networks, exploring questions on training dynamics and generalization. Additionally, she has driven the development of machine learning algorithms for use in computational science, including solution methods for complicated PDE problems and techniques for incorporating mathematical models into data-driven approaches. 
She has spearheaded the development of tensor algorithms, first proposed in her seminal paper, which enable efficient processing of multidimensional and multimodal data. Her contributions to Physics Informed Deep Learning, such as the Fourier Neural Operator network for solving parametric PDEs, have sparked great interest in the community.

## Organizers


